name: local-llm

services:
  ollama:
    image: ollama/ollama:latest
    platform: linux/amd64 # Use amd64 for better compatibility
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    # Auto-download Llama 3.2 1B model on first startup
    command: >
      sh -c "ollama serve &
             OLLAMA_PID=$$!
             echo 'Waiting for Ollama server to start...'
             until curl -f http://localhost:11434/api/tags >/dev/null 2>&1; do
               sleep 2
             done
             echo 'Ollama server is ready, downloading model...'
             ollama pull llama3.2:1b
             echo 'Model download completed'
             wait $$OLLAMA_PID"

  api:
    image: ghcr.io/inderjotpujara/localai-privacy-api:latest
    environment:
      - LLM_BASE_URL=http://ollama:11434/v1
      - PORT=3000
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "3000:3000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "3000"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 30s

volumes:
  ollama:
