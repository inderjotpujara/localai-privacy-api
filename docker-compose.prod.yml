name: local-llm

services:
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "ps"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Model downloader - runs once to download the model
  model-downloader:
    image: ollama/ollama:latest
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "echo 'Checking if Llama 3.2 1B model is already available...' &&
       if ollama list | grep -q 'llama3.2:1b'; then
         echo 'Model llama3.2:1b already exists, skipping download.';
       else
         echo 'Model not found, downloading Llama 3.2 1B model...' &&
         ollama pull llama3.2:1b &&
         echo 'Model download completed!';
       fi"
    restart: "no"

  api:
    image: ghcr.io/inderjotpujara/localai-privacy-api:latest
    environment:
      - LLM_BASE_URL=http://ollama:11434/v1
      - PORT=3000
    depends_on:
      model-downloader:
        condition: service_completed_successfully
    ports:
      - "3000:3000"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "3000"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 30s

volumes:
  ollama:
