# Local LLM API FaÃ§ade

A privacy-first Node.js (TypeScript) API faÃ§ade that proxies LocalAI running Llama-3 on Apple Silicon, with streaming chat, RAG capabilities, JWT auth, and full observability stack.

## Features

- ğŸ¤– **Chat Endpoint**: `/chat` with streaming support (SSE) and regular responses
- ğŸ” **RAG Query**: `/rag/query` for vector-based semantic search
- ğŸ” **JWT Authentication**: Local token verification
- ğŸ  **Privacy-First**: All data, embeddings, and logs remain local
- ğŸ“Š **Observability**: Loki, Promtail, Grafana stack
- ğŸ³ **Docker Compose**: Full orchestration
- ğŸš€ **CI/CD**: Multi-arch Docker builds (arm64/amd64)
- ğŸ **Apple Metal**: Optional native acceleration

## Quick Start

1. **Clone and Setup**
   ```bash
   git clone <repo-url>
   cd local-llm
   cp .env.sample .env
   # Edit .env with your configurations
   ```

2. **Start Services**
   ```bash
   docker-compose up -d
   ```

3. **Test Chat**
   ```bash
   curl -X POST http://localhost:3000/chat \
     -H "Authorization: Bearer your-jwt-token" \
     -H "Content-Type: application/json" \
     -d '{"message": "Hello, world!"}'
   ```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client App    â”‚ â”€â”€â–¶â”‚   Node.js API   â”‚ â”€â”€â–¶â”‚    LocalAI      â”‚
â”‚                 â”‚    â”‚   (TypeScript)  â”‚    â”‚   (Llama-3)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Postgres+pgvectorâ”‚
                       â”‚   (RAG Store)   â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Loki+Grafana    â”‚
                       â”‚ (Observability) â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## API Endpoints

### Chat
```bash
# Regular chat
POST /chat
{
  "message": "Your message",
  "stream": false
}

# Streaming chat (SSE)
POST /chat
{
  "message": "Your message", 
  "stream": true
}
```

### RAG Query
```bash
POST /rag/query
{
  "query": "Search query",
  "limit": 5
}
```

## Development

```bash
cd api
npm install
npm run dev
```

## Docker Hub

Multi-arch images are automatically built and pushed:
- `your-username/local-llm:latest`
- `your-username/local-llm:${GITHUB_SHA}`

## Privacy Guarantees

| Component | Data Location | External Calls |
|-----------|---------------|----------------|
| Chat Messages | Local only | None |
| Vector Embeddings | Local pgvector | None |
| Auth Tokens | Local verification | None |
| Logs/Metrics | Local Loki/Grafana | None |
| Model Weights | Local Docker volumes | None |

## Apple Silicon Notes

For optimal performance on Apple Silicon:
1. Use LocalAI with Metal acceleration
2. Set `LOCALAI_BACKEND=metal` in `.env`
3. Ensure Docker Desktop uses Apple Silicon images

## Contributing

1. Make atomic commits with clear messages
2. All data must remain local
3. No external API dependencies
4. Test both streaming and regular endpoints

## License

MIT
